{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank-Additional DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Selecting the Business Problem\n",
    "\n",
    "### Identify Business Goals\n",
    "The primary objective of the Portuguese banking institution is to increase the subscription rate of term deposits through their direct marketing campaigns. By enhancing the effectiveness of these campaigns, the bank aims to:\n",
    "\n",
    "- Boost overall revenue.\n",
    "- Expand its customer base.\n",
    "- Strengthen market position.\n",
    "- Improve customer relationships and satisfaction.\n",
    "\n",
    "### Align with Data Availability\n",
    "The **Bank Marketing Dataset** provides comprehensive data that aligns with this business goal:\n",
    "\n",
    "- **Customer Demographics**: `age`, `job`, `marital`, `education`.\n",
    "- **Financial Information**: `balance`, `default` (credit in default), `housing` (housing loan), `loan` (personal loan).\n",
    "- **Marketing Interaction Details**: `contact` (communication type), `day` (last contact day), `month` (last contact month), `duration` (last contact duration).\n",
    "- **Previous Campaign Outcomes**: `campaign` (number of contacts), `pdays` (days since last contact), `previous` (number of prior contacts), `poutcome` (outcome of the previous campaign).\n",
    "\n",
    "This rich dataset enables the bank to analyze customer behaviors and preferences, which is essential for tailoring marketing strategies effectively.\n",
    "\n",
    "### Specific Problem Statement\n",
    "How can we segment our customers based on their demographic characteristics, financial status, and previous interactions to tailor our marketing campaigns effectively and increase the subscription rate of term deposits?\n",
    "\n",
    "---\n",
    "\n",
    "### Importance of Segmentation\n",
    "Customer segmentation is crucial for businesses due to the following reasons:\n",
    "\n",
    "1. **Personalized Marketing**: Enables the creation of targeted marketing messages that resonate with specific groups, increasing engagement and conversion rates.\n",
    "2. **Improved Customer Service**: Allows businesses to address the unique needs and concerns of different customer segments, enhancing satisfaction and loyalty.\n",
    "3. **Optimized Resource Allocation**: Helps in allocating marketing budgets and resources more efficiently by focusing on high-potential segments.\n",
    "4. **Product Development**: Insights from segmentation can guide the development of new products or services tailored to the needs of specific groups.\n",
    "5. **Competitive Advantage**: Understanding customer segments better than competitors can lead to more effective strategies and increased market share.\n",
    "\n",
    "---\n",
    "\n",
    "### Specific Benefits for the Bank Marketing Dataset\n",
    "Applying customer segmentation to the Bank Marketing Dataset offers several benefits:\n",
    "\n",
    "1. **Enhanced Campaign Effectiveness**: By identifying distinct customer segments, the bank can tailor its communication strategies to address the specific needs and preferences of each group, leading to higher subscription rates for term deposits.\n",
    "2. **Increased Conversion Rates**: Targeting customers who are more likely to subscribe based on their profile and past behavior can improve conversion rates and reduce the cost per acquisition.\n",
    "3. **Customer Retention**: Understanding different customer segments helps in developing retention strategies for valuable customers, such as offering personalized financial advice or exclusive products.\n",
    "4. **Resource Optimization**: Focusing efforts on segments with the highest potential return allows for better use of marketing resources and maximizes ROI.\n",
    "5. **Strategic Planning**: Insights from segmentation can inform broader business strategies, such as identifying new market opportunities or adjusting product offerings to meet the needs of underserved segments.\n",
    "\n",
    "By leveraging clustering techniques on this dataset, the bank can gain valuable insights into customer behaviors and preferences, ultimately leading to more successful marketing campaigns and business growth.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   **INPUT VARIABLES:**\n",
    "\n",
    "   **Bank client data:**\n",
    "\n",
    "   - <ins>age</ins> (numeric)\n",
    "   - <ins>job</ins> : type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n",
    "   - <ins>marital</ins> : marital status (categorical: \"divorced\",\"married\",\"single\",\"unknown\"; note: \"divorced\" means divorced or widowed)\n",
    "   - <ins>education</ins> (categorical: \"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"illiterate\",\"professional.course\",\"university.degree\",\"unknown\")\n",
    "   - <ins>default</ins>: has credit in default? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   - <ins>housing</ins>: has housing loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   - <ins>loan</ins>: has personal loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "\n",
    "   **Related with the last contact of the current campaign:**\n",
    "\n",
    "   - <ins>contact</ins>: contact communication type (categorical: \"cellular\",\"telephone\") \n",
    "   - <ins>month</ins>: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "   - <ins>day_of_week</ins>: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n",
    "   - <ins>duration</ins>: last contact duration, in seconds (numeric). Important note:  this attribute highly affects the output target (e.g., if duration=0 then y=\"no\"). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "   **Other attributes:**\n",
    "\n",
    "   - <ins>campaign</ins>: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "   - <ins>pdays</ins>: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "   - <ins>previous</ins>: number of contacts performed before this campaign and for this client (numeric)\n",
    "   - <ins>poutcome</ins>: outcome of the previous marketing campaign (categorical: \"failure\",\"nonexistent\",\"success\")\n",
    "   **Social and economic context attributes:**\n",
    "   - <ins>emp.var.rate</ins>: employment variation rate - quarterly indicator (numeric)\n",
    "   - <ins>cons.price.idx</ins>: consumer price index - monthly indicator (numeric)     \n",
    "   - <ins>cons.conf.idx</ins>: consumer confidence index - monthly indicator (numeric)     \n",
    "   - <ins>euribor3m</ins>: euribor 3 month rate - daily indicator (numeric)\n",
    "   - <ins>nr.employed</ins>: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "  **OUTPUT VARIABLE (DESIRED TARGET):**\n",
    "\n",
    "   - <ins>y</ins> - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sl\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.vq import vq\n",
    "\n",
    "# Load the dataset\n",
    "ds = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the structure of the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "ds.info()\n",
    "\n",
    "\n",
    "# Get summary statistics for numerical features\n",
    "print(\"\\nSummary Statistics for Numerical Features:\")\n",
    "display(ds.describe())\n",
    "\n",
    "# Get summary statistics for categorical features\n",
    "print(\"\\nSummary Statistics for Categorical Features:\")\n",
    "display(ds.describe(include='object'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Feature Distributions\n",
    "# List of numerical and categorical features\n",
    "numerical_features = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "                      'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "                        'contact', 'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "# Histograms for numerical features with multiple plots per row\n",
    "plots_per_row = 3  # Number of plots per row\n",
    "num_features = len(numerical_features)\n",
    "\n",
    "for i in range(0, num_features, plots_per_row):\n",
    "    features = numerical_features[i:i+plots_per_row]\n",
    "    num_plots = len(features)\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 4))\n",
    "    \n",
    "    if num_plots == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable when there's only one plot\n",
    "    \n",
    "    for ax, feature in zip(axes, features):\n",
    "        sns.histplot(ds[feature], bins=30, kde=True, ax=ax)\n",
    "        ax.set_title(f'Distribution of {feature}')\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Bar plots for categorical features with multiple plots per row\n",
    "num_features = len(categorical_features)\n",
    "\n",
    "for i in range(0, num_features, plots_per_row):\n",
    "    features = categorical_features[i:i+plots_per_row]\n",
    "    num_plots = len(features)\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 4))\n",
    "    \n",
    "    if num_plots == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable when there's only one plot\n",
    "    \n",
    "    for ax, feature in zip(axes, features):\n",
    "        sns.countplot(data=ds, x=feature, order=ds[feature].value_counts().index, ax=ax)\n",
    "        ax.set_title(f'Distribution of {feature}')\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns and Anomalies\n",
    "\n",
    "#### Outliers\n",
    "- **Duration**: The long tail suggests potential outliers with exceptionally long calls.\n",
    "- **Pdays**: The peak at 999 indicates a placeholder value for customers not previously contacted, which could skew analyses.\n",
    "- **Campaign**: A few cases with very high contact numbers might be outliers.\n",
    "\n",
    "#### Missing Values\n",
    "- Categorical features like `job`, `education`, `default`, `housing`, and `loan` contain `\"unknown\"` values. These could represent missing data that needs imputation or separate treatment.\n",
    "\n",
    "#### Patterns\n",
    "- Economic indicators like `emp.var.rate` and `cons.price.idx` show distinct clusters, possibly corresponding to different economic conditions.\n",
    "- The concentration of `age` around 30–50 and `duration` below 500 seconds suggests a targeted customer group and the brevity of most marketing interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values in Each Column:\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# Initialize a list to store missing value information\n",
    "missing_values = []\n",
    "\n",
    "# Check for missing values in numerical features\n",
    "for feature in numerical_features:\n",
    "    missing_count = ds[feature].isnull().sum()\n",
    "    total_count = ds.shape[0]\n",
    "    missing_percentage = (missing_count / total_count) * 100\n",
    "    missing_values.append({'Feature': feature,\n",
    "                           'Missing Count': missing_count,\n",
    "                           'Missing Percentage': missing_percentage})\n",
    "\n",
    "# Check for missing values in categorical features (considering 'unknown' as missing)\n",
    "for feature in categorical_features:\n",
    "    missing_count = ds[ds[feature] == 'unknown'][feature].count()\n",
    "    total_count = ds.shape[0]\n",
    "    missing_percentage = (missing_count / total_count) * 100\n",
    "    missing_values.append({'Feature': feature,\n",
    "                           'Missing Count': missing_count,\n",
    "                           'Missing Percentage': missing_percentage})\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "missing_value_df = pd.DataFrame(missing_values)\n",
    "\n",
    "# Print the missing value information\n",
    "print(missing_value_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding the contact Feature from Clustering Analysis\n",
    "\n",
    "Excluding the contact feature from the clustering process ensures that the resulting segments are more meaningful and unbiased. By removing this operational variable, the analysis focuses on more intrinsic attributes, allowing the clusters to better reflect natural patterns in the data rather than being influenced by the bank's communication methods. This approach enhances the clarity and actionability of the segments, making them more suitable for developing targeted strategies and driving effective marketing decisions. For the features with low missing percentages (job, marital, education, housing, loan) we substitute the unkown values with the mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'contact' feature from the dataset\n",
    "ds = ds.drop(columns=['contact'])\n",
    "\n",
    "# List of features to impute\n",
    "features_to_impute = ['job', 'marital', 'education', 'housing', 'loan']\n",
    "\n",
    "for feature in features_to_impute:\n",
    "    mode_value = ds[ds[feature] != 'unknown'][feature].mode()[0]\n",
    "    ds.loc[ds[feature] == 'unknown', feature] = mode_value\n",
    "    print(f'Imputed \"unknown\" in {feature} with mode: {mode_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers handling\n",
    "\n",
    "\n",
    "#### 1. pdays feature\n",
    "- Create a binary feature indicating whether the customer was previously contacted.\n",
    "- Set pdays to NaN or another placeholder value for customers not previously contacted.\n",
    "\n",
    "#### 2. duration feature\n",
    "- Winsorize outliers by capping extreme values at the 99th percentile to reduce their impact while retaining all data points.\n",
    "\n",
    "#### 3. campaign feature\n",
    "- Cap values at the 95th percentile to address extreme cases and improve consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handling outliers in 'duration'\n",
    "duration_99 = ds['duration'].quantile(0.99)\n",
    "print(f\"99th percentile of 'duration': {duration_99} seconds\")\n",
    "\n",
    "ds['duration_capped'] = np.where(ds['duration'] > duration_99, duration_99, ds['duration'])\n",
    "\n",
    "# Visualize 'duration_capped'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(ds['duration_capped'], bins=30, kde=True)\n",
    "plt.title('Distribution of Duration after Capping Outliers')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Handling 'pdays'\n",
    "\n",
    "ds['previously_contacted'] = np.where(ds['pdays'] != 999, 1, 0)\n",
    "ds['pdays_cleaned'] = ds['pdays'].replace(999, -1)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize 'pdays_cleaned'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(ds['pdays_cleaned'], bins=30, kde=True)\n",
    "plt.title('Distribution of pdays after Cleaning')\n",
    "plt.xlabel('pdays')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Handling outliers in 'campaign'\n",
    "campaign_95 = ds['campaign'].quantile(0.95)\n",
    "print(f\"95th percentile of 'campaign': {campaign_95}\")\n",
    "\n",
    "ds['campaign_capped'] = np.where(ds['campaign'] > campaign_95, campaign_95, ds['campaign'])\n",
    "\n",
    "# Visualize 'campaign_capped'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(ds['campaign_capped'], bins=30, kde=True)\n",
    "plt.title('Distribution of Campaign after Capping Outliers')\n",
    "plt.xlabel('Number of Contacts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "ds = ds.drop(columns=['pdays', 'duration', 'campaign'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n",
    "In this step, categorical features are transformed into numeric format using LabelEncoder. This encoding ensures that machine learning models can process these features effectively. Each categorical feature is replaced with its corresponding numerical representation, making the dataset ready for further analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# List of categorical features to encode\n",
    "# List of categorical features to encode (without 'contact')\n",
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "                        'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "\n",
    "# Encode each categorical feature using LabelEncoder\n",
    "for feature in categorical_features:\n",
    "    ds[feature] = label_encoder.fit_transform(ds[feature])\n",
    "    print(f\"Encoded '{feature}' with classes: {label_encoder.classes_}\")\n",
    "\n",
    "display(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Encoding Age Groups\n",
    "This code dynamically defines age bins based on the dataset's minimum and maximum age, categorizes ages into groups (`young`, `adult`, `senior`), and encodes the `age_group` feature using LabelEncoder. Adjusting bins ensures all ages are included, and the encoding maps groups to numerical values for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age bins and labels, adjusting to include all ages\n",
    "min_age = ds['age'].min()\n",
    "max_age = ds['age'].max()\n",
    "\n",
    "age_bins = [min_age, 30, 60, max_age + 1]\n",
    "age_labels = ['young', 'adult', 'senior']\n",
    "\n",
    "# Create the 'age_group' feature\n",
    "ds['age_group'] = pd.cut(ds['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Check for missing values in 'age_group'\n",
    "missing_age_groups = ds['age_group'].isnull().sum()\n",
    "print(f\"Missing age groups: {missing_age_groups}\")\n",
    "\n",
    "# If there are missing values, decide how to handle them\n",
    "# For this example, we'll assume there are no missing values after adjusting the bins\n",
    "\n",
    "# Encode 'age_group' using LabelEncoder\n",
    "ds['age_group'] = ds['age_group'].astype(str)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "ds['age_group_encoded'] = label_encoder.fit_transform(ds['age_group'])\n",
    "\n",
    "# Display the mapping\n",
    "age_group_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Age Group Encoding Mapping:\")\n",
    "print(age_group_mapping)\n",
    "\n",
    "# Verify the distribution\n",
    "print(\"Age Group Distribution:\")\n",
    "print(ds['age_group'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features normalization\n",
    "Normalization ensures that all numerical features contribute equally to the analysis by standardizing them to a common scale. In an unsupervised learning context, this is crucial for distance-based algorithms (e.g., clustering), where features with larger scales could dominate the results and bias the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_features = ['age', 'duration_capped', 'campaign_capped', 'pdays_cleaned', 'previous', 'emp.var.rate', \n",
    "                      'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "# Check for missing values in the dataset\n",
    "print(\"Missing values in the dataset before scaling:\")\n",
    "print(ds.isnull().sum())\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the numerical features\n",
    "scaler.fit(ds[numerical_features])\n",
    "\n",
    "# Transform the numerical features\n",
    "ds[numerical_features] = scaler.transform(ds[numerical_features])\n",
    "# Drop the 'age_group' column from the dataset\n",
    "\n",
    "ds = ds.drop(columns=['age_group'])\n",
    "\n",
    "\n",
    "print(ds.dtypes)\n",
    "display(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encode the target variable 'y' into numerical format\n",
    "ds['num_y'] = ds['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# 2. Drop the original 'y' column\n",
    "ds = ds.drop(columns=['y'])\n",
    "\n",
    "# 3. Separate features and labels\n",
    "X = ds.drop(columns=['num_y'])\n",
    "y = ds['num_y']\n",
    "\n",
    "# 4. Reset index to ensure alignment\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# 5. Separate the data based on the target variable\n",
    "Si_sample = X.loc[y == 1].reset_index(drop=True)\n",
    "No_sample = X.loc[y == 0].reset_index(drop=True)\n",
    "\n",
    "# 6. Combine Si and No samples for consistent scaling\n",
    "combined_data = pd.concat([Si_sample, No_sample], ignore_index=True)\n",
    "\n",
    "# **New Step: Encode non-numeric columns**\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_columns = combined_data.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
    "\n",
    "# Encode non-numeric columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    combined_data[col] = label_encoder.fit_transform(combined_data[col])\n",
    "\n",
    "# 7. Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 8. Fit and transform the combined data\n",
    "scaled_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# 9. Apply PCA to the combined scaled data\n",
    "n_components = 2  # Number of principal components\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_transformed = pca.fit_transform(scaled_data)\n",
    "\n",
    "# 10. Get the number of samples in each group\n",
    "num_Si_samples = len(Si_sample)\n",
    "num_No_samples = len(No_sample)\n",
    "\n",
    "# 11. Separate the transformed data back into Si and No samples\n",
    "Si_transformed = pca_transformed[:num_Si_samples]\n",
    "No_transformed = pca_transformed[num_Si_samples:]\n",
    "\n",
    "# 12. Plot the first two principal components for Si and No samples\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(No_transformed[:, 0], No_transformed[:, 1], color='red', label='NO', alpha=0.6)\n",
    "plt.scatter(Si_transformed[:, 0], Si_transformed[:, 1], color='green', label='YES', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA: First Two Principal Components\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 13. Print explained variance ratio for PC1 and PC2\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio for PC1 and PC2: {explained_variance}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretation of PCA Results**\n",
    "The PCA plot shows the data reduced to two dimensions (PC1 and PC2), where PC1 captures the most variance and PC2 captures the second most. \n",
    "The partial overlap between the `YES` (green) and `NO` (red) groups indicates that the features used for PCA do not fully separate the two classes, suggesting shared characteristics or noise in the data. \n",
    "Some distinct clusters are visible, highlighting subsets of the data with unique feature combinations. Overall, while PCA reveals variability and some patterns, additional components or refined features may be needed for better group differentiation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can try to calculate the cumulative explained variance for all principal components using PCA, showing how much variance is retained as components are added.  \n",
    "The cumulative variance is plotted to visualize the contribution of each component, helping identify the optimal number of components needed to retain most of the dataset's information.  \n",
    "The plot provides a clear way to determine the point of diminishing returns, where adding more components yields minimal additional variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate explained variance for all components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with the chosen number of components\n",
    "n_components = 6  # or 8, based on your preference\n",
    "pca = PCA(n_components=n_components)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "# Get the PCA components (loadings)\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i}' for i in range(1, n_components + 1)],\n",
    "    index=X.columns\n",
    ")\n",
    "\n",
    "# Display loadings\n",
    "print(\"PCA Loadings:\")\n",
    "display(loadings)\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine the reduced data with the target variable\n",
    "df_pca = pd.DataFrame(X_reduced, columns=[f'PC{i}' for i in range(1, n_components + 1)])\n",
    "df_pca['Target'] = y.values\n",
    "\n",
    "# Plot pairwise relationships\n",
    "sns.pairplot(df_pca, hue='Target', vars=[f'PC{i}' for i in range(1, 4)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA Results**\n",
    "\n",
    "The PCA analysis reduces the dataset into components, with PC1, PC2, and PC3 capturing the most variance. The pairwise scatterplots show significant overlap between the two classes (`0` and `1`), indicating limited class separability in this space.  \n",
    "\n",
    "PCA loadings reveal that `job`, `month`, and `education` dominate the first three components, highlighting their importance in explaining the variance. Additional feature engineering or non-linear dimensionality reduction (e.g., t-SNE) may improve class separation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Assuming 'X_reduced' is your PCA-reduced data with 6 components\n",
    "# and 'y' is your target variable\n",
    "\n",
    "# 1. Determine the optimal number of clusters using the Elbow Method\n",
    "wcss = []\n",
    "K = range(1, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_reduced)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, wcss, 'bo-', markersize=8)\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elbow Method for Optimal k**\n",
    "\n",
    "This graph uses the Elbow Method to determine the optimal number of clusters (k) for K-Means clustering. The x-axis represents the number of clusters, and the y-axis shows the Within-Cluster Sum of Squares (WCSS), which measures the total variance within clusters.\n",
    "\n",
    "The \"elbow\" point occurs around **k=4 or k=5**, where the reduction in WCSS starts to slow down significantly. This suggests that 4 or 5 clusters may be optimal, as adding more clusters does not provide substantial improvement in reducing variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not perform Silhoutte analysis on the whole dataset due to his size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 5 \n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "df_clusters = pd.DataFrame(X_reduced, columns=[f'PC{i}' for i in range(1, n_components + 1)])\n",
    "df_clusters['Cluster'] = cluster_labels\n",
    "df_clusters['Target'] = y.values\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='Cluster',\n",
    "    data=df_clusters,\n",
    "    palette='viridis',\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title(f'K-Means Clusters Visualization (k={optimal_k})')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(\n",
    "    df_clusters['PC1'],\n",
    "    df_clusters['PC2'],\n",
    "    df_clusters['PC3'],\n",
    "    c=df_clusters['Cluster'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.6\n",
    ")\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title(f'3D K-Means Clusters (k={optimal_k})')\n",
    "legend = ax.legend(*scatter.legend_elements(), title='Cluster')\n",
    "ax.add_artist(legend)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
